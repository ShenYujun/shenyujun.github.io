<!DOCTYPE html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Yujun Shen (Damon)'s homepage.">
  <meta name="keywords" content="Yujun Shen, Computer Vision, GAN">
  <meta name="author" content="Yujun Shen">

  <title>Yujun Shen (Damon)</title>

  <link rel="stylesheet" type="text/css" href="assets/style.css">
  <link rel="icon" type="image/png" href="assets/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="assets/large-icon.png">
</head>
<!-- === Header Ends === -->


<body data-new-gr-c-s-check-loaded="14.1014.0" data-gr-ext-installed="">


<!-- === Homepage Starts === -->
<table width="980px" align="center" border="0">
<tbody>
<tr>


<td></td>  <!-- Leave one column blank on the left. -->
<td valign="top">


<!-- === Avatar Starts === -->
<br>
<table style="font-size: 12pt;" width="100%" border="0">
<tbody>
  <tr>
    <td width="60%">
      <p style="font-size: 20pt;">
        <b>Yujun Shen (Damon)</b>
      </p>
      <p style="text-align: justify;">
        Yujun Shen is currently a senior research scientist at Ant Research, leading the Interatcion Intelligence Lab.
        His research focuses on computer vision and deep learning, particularly on <b>generative models</b> and <b>3D vision</b>.
      </p>
      <br>
      <a href="https://scholar.google.com/citations?user=u76xfogAAAAJ" target="_blank">Google scholar</a>&nbsp;&nbsp;/&nbsp;
      <a class="icon" href="https://github.com/ShenYujun/" target="_blank">Github</a>&nbsp;&nbsp;/&nbsp;
      <a class="icon" href="https://www.linkedin.com/in/yujun-shen-58b74393/" target="_blank">LinkedIn</a>&nbsp;&nbsp;/&nbsp;
      <a class="icon" href="mailto:shenyujun0302@gmail.com" target="_blank">Email</a>
    </td>
    <td width="12%"></td>
    <td width="30%">
      <img width="250" src="./assets/me.jpg">
    </td>
  </tr>
</tbody>
</table>
<!-- === Avatar Ends === -->


<!-- === Biography Starts === -->
<!-- <br>
<h2>Biography</h2>
<p style="text-align: justify;">
  Yujun Shen is currently a senior research scientist at Ant Research, leading the Interatcion Intelligence Lab.
  His research focuses on computer vision and deep learning, particularly on <b>generative models</b> and <b>3D vision</b>.
</p> -->
<!-- === Biography Ends === -->


<!-- === News Starts === -->
<br>
<h2>News</h2>
<ul>
  <li><strong>[09/2023]</strong> 8 papers, including 2 Datasets & Benchmarks Track, accepted to <a href="https://nips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a> (New Orleans, U.S.).</li>
  <li><strong>[08/2023]</strong> 1 paper, including 1 journal, accepted to <a href="https://asia.siggraph.org/2023/" target="_blank">SIGGRAPH Asia 2023</a> (Sydney, Australia).</li>
  <li><strong>[08/2023]</strong> 1 paper (<a href="https://arxiv.org/pdf/2309.13956.pdf" target="_blank">IDInvert</a>) included in TPAMI 2023.</li>
  <li><strong>[07/2023]</strong> 5 papers accepted to <a href="https://iccv2023.thecvf.com/" target="_blank">ICCV 2023</a> (Paris, France).</li>
  <li><strong>[04/2023]</strong> 1 paper accepted to <a href="https://icml.cc/Conferences/2023" target="_blank">ICML 2023</a> (Hawaii, U.S.).</li>
  <li><strong>[03/2023]</strong> 8 papers, including 1 highlight, accepted to <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a> (Vancouver, Canada).</li>
  <li><strong>[01/2023]</strong> 1 paper accepted to <a href="https://iclr.cc/Conferences/2023" target="_blank">ICLR 2023</a> (Kigali, Rwanda).</li>
  <li><strong>[10/2022]</strong> 1 paper (<a href="https://arxiv.org/pdf/2301.05315.pdf" target="_blank">GH-Feat</a>) included in <a href="https://ieeexplore.ieee.org/document/9968154" target="_blank">TPAMI 2022</a>.</li>
  <li><strong>[09/2022]</strong> 4 papers, including 2 spotlight, accepted to <a href="https://nips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a> (New Orleans, U.S.).</li>
  <li><strong>[07/2022]</strong> 2 papers, including 1 oral, accepted to <a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a> (Tel Aviv, Israel).</li>
  <li><strong>[05/2022]</strong> 1 paper accepted to <a href="https://icml.cc/Conferences/2022" target="_blank">ICML 2022</a> (Baltimore, U.S.).</li>
  <li><strong>[03/2022]</strong> 4 papers, including 1 oral, accepted to <a href="https://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a> (New Orleans, U.S.).</li>
  <li><strong>[09/2021]</strong> 2 papers accepted to <a href="https://nips.cc/Conferences/2021" target="_blank">NeurIPS 2021</a> (Virtual).</li>
  <li><strong>[03/2021]</strong> 5 papers, including 2 oral and 2 workshop, accepted to <a href="https://cvpr2021.thecvf.com/" target="_blank">CVPR 2021</a> (Virtual).</li>
  <li><strong>[12/2020]</strong> 1 paper (<a href="https://arxiv.org/pdf/1911.09267.pdf" target="_blank">HiGAN</a>) included in <a href="https://link.springer.com/article/10.1007/s11263-020-01429-5" target="_blank">IJCV 2021</a>.</li>
  <li><strong>[10/2020]</strong> 1 paper (<a href="https://arxiv.org/pdf/2005.09635.pdf" target="_blank">InterFaceGAN</a>) included in <a href="https://ieeexplore.ieee.org/document/9241434" target="_blank">TPAMI 2020</a>.</li>
  <li><strong>[07/2020]</strong> 1 paper accepted to <a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a> (Virtual).</li>
  <li><strong>[02/2020]</strong> 2 papers accepted to <a href="https://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a> (Virtual).</li>
  <!-- <li><strong>[03/2018]</strong> 1 paper accepted to <a href="https://cvpr2018.thecvf.com/" target="_blank">CVPR 2018 (Salt Lake City, U.S.)</a>.</li> -->
</ul>
<!-- === News Ends === -->


<!-- === Software Starts === -->
<br>
<h2>Softwares</h2>
<ul>

  <li><strong>[09/2023]</strong>
    We release
    <a href="https://github.com/zhujiapeng/Aurora" target="_blank">Aurora</a>,
    an open-sourced GAN-based text-to-image generation model.
  </li>

  <li><strong>[08/2023]</strong>
    We release
    <a href="https://github.com/qiuyu96/CoDeF" target="_blank">CoDeF</a>,
    a novel representation for temporally consistent video processing.
  </li>

  <li><strong>[07/2023]</strong>
    We release
    <a href="https://github.com/qiuyu96/carver" target="_blank">Carver</a>,
    a highly modularized codebase for 3D-aware image synthesis.
  </li>

  <!-- <li><strong>[07/2023]</strong>
    We release
    <a href="https://github.com/vivianszf/pof3d" target="_blank">PoF3D</a>,
    <a href="https://github.com/snap-research/discoscene" target="_blank">DiscoScene</a>,
    <a href="https://github.com/vivianszf/geod" target="_blank">GeoD</a>,
    <a href="https://github.com/vivianszf/depthgan" target="_blank">DepthGAN</a>,
    <a href="https://github.com/genforce/volumegan" target="_blank">VolumeGAN</a>,
    to facilitate 3D-aware image synthesis.
  </li> -->

  <!-- <li><strong>[06/2023]</strong>
    We release
    <a href="https://github.com/damo-vilab/videocomposer" target="_blank">VideoComposer</a>,
    <a href="https://github.com/damo-vilab/composer" target="_blank">Composer</a>,
    following a compositional paradigm for content creation.
  </li> -->

  <!-- <li><strong>[06/2023]</strong>
    We release
    <a href="https://github.com/genforce/StyleSV" target="_blank">StyleSV</a>,
    a strong baseline for video generation.
  </li> -->

  <!-- <li><strong>[06/2023]</strong>
    We release
    <a href="https://github.com/EzioBy/glead" target="_blank">GLeaD</a>,
    <a href="https://github.com/genforce/dynamicd" target="_blank">DynamicD</a>,
    <a href="https://github.com/genforce/eqgan" target="_blank">EqGAN</a>,
    <a href="https://github.com/genforce/insgen" target="_blank">InsGen</a>,
    to facilitate GAN training.
  </li> -->

  <!-- <li><strong>[10/2022]</strong>
    We public a
    <a href="https://github.com/justimyhxu/awesome-3D-generation" target="_blank">collection of studies</a>
    on 3D generative models.
  </li> -->

  <!-- <li><strong>[07/2022]</strong>
    We release
    <a href="https://github.com/ezioby/padinv" target="_blank">PadInv</a>,
    <a href="https://github.com/genforce/ghfeat" target="_blank">GH-Feat</a>,
    <a href="https://github.com/genforce/idinvert" target="_blank">IDInvert</a>,
    to facilitate GAN inversion and its applications.
  </li> -->

  <!-- <li><strong>[06/2022]</strong>
    We release
    <a href="https://github.com/zhujiapeng/resefa" target="_blank">ReSeFa</a>,
    <a href="https://github.com/zhujiapeng/lowrankgan" target="_blank">LowRankGAN</a>,
    <a href="https://github.com/genforce/sefa" target="_blank">SeFa</a>,
    <a href="https://github.com/genforce/higan" target="_blank">HiGAN</a>,
    <a href="https://github.com/genforce/interfacegan" target="_blank">InterFaceGAN</a>,
    to facilitate interpretation of generative models.
  </li> -->

  <li><strong>[02/2022]</strong>
    We release
    <a href="https://github.com/bytedance/Hammer" target="_blank">Hammer</a>,
    a sufficiently upgraded version of GenForce, to facilitate the training of deep models.
  </li>

  <li><strong>[09/2020]</strong>
    We release
    <a href="https://github.com/genforce/genforce" target="_blank">GenForce</a>,
    an efficient PyTorch library for deep generative modeling.
  </li>

</ul>
<!-- === Software Ends === -->


<!-- === Publication Starts === -->
<br>
<h2>
  Recent Studies
  <small>[<a href="./0_publication_selected.html">Publication List</a>]</small>
</h2>

<table cellspacing="17">
<tbody>

<tr name="CoDeF"></tr>
<tr name="AGAP"></tr>
<tr name="AnyDoor"></tr>
<tr name="LivePhoto"></tr>
<tr name="Ranni"></tr>
<tr name="Aurora"></tr>
<tr name="4K4D"></tr>
<tr name="GenDeF"></tr>
<tr name="BerfScene"></tr>
<tr name="SMaRt"></tr>
<tr name="3D Survey"></tr>
<tr name="ScaNeRF"></tr>
<tr name="Carver"></tr>
<tr name="VideoComposer"></tr>
<tr name="Composer"></tr>

</tbody>
</table>
<!-- === Publication Ends === -->


<!-- === Experience Starts === -->
<br>
<h2>Experiences</h2>

<table cellspacing="17">
<tbody>

  <tr>
    <td width="8%">
      <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./assets/institutions/ant.png">
    </td>
    <td>
      <div class="institution">Ant Group</div>
      <div class="period">Apr. 2022 - Present</div>
      <div class="position">Senior Research Scientist</div>
    </td>
  </tr>

  <tr>
    <td width="8%">
      <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./assets/institutions/bytedance.png">
    </td>
    <td>
      <div class="institution">ByteDance</div>
      <div class="period">Mar. 2021 - Apr. 2022</div>
      <div class="position">Senior Researcher</div>
    </td>
  </tr>

  <tr>
    <td width="8%">
      <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./assets/institutions/google.png">
    </td>
    <td>
      <div class="institution">Google</div>
      <div class="period">Nov. 2019 - May 2020</div>
      <div class="position">Software Engineer Intern</div>
    </td>
  </tr>

  <tr>
    <td width="8%">
      <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./assets/institutions/facebook.png">
    </td>
    <td>
      <div class="institution">Facebook Reality Lab (FRL)</div>
      <div class="period">May 2019 - Sep. 2019</div>
      <div class="position">Research Intern</div>
    </td>
  </tr>

  <tr>
    <td width="8%">
      <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./assets/institutions/google.png">
    </td>
    <td>
      <div class="institution">Google</div>
      <div class="period">Jul. 2018 - Dec. 2018</div>
      <div class="position">Software Engineer Intern</div>
    </td>
  </tr>

  <tr>
    <td width="8%">
      <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./assets/institutions/cuhk.png">
    </td>
    <td>
      <div class="institution">The Chinese University of Hong Kong</div>
      <div class="period">Aug. 2016 - May. 2021</div>
      <div class="position">Ph.D. Degree in Information Engineering</div>
    </td>
  </tr>

  <tr>
    <td width="8%">
      <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./assets/institutions/tsinghua.png">
    </td>
    <td>
      <div class="institution">Tsinghua University</div>
      <div class="period">Aug. 2012 - Jul. 2016</div>
      <div class="position">Bachelor Degree in Electronic Engineering<br>Second Bachelor Degree in Management</div>
    </td>
  </tr>

</tbody>
</table>
<!-- === Experience Ends === -->


<!-- === Activity Starts === -->
<br>
<h2>Professional Activities</h2>
<ul>
  <li>Conference reviewer: CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, SIGGRAPH, SIGGRAPH Asia, AAAI</li>
  <li>Journal reviewer: TPAMI, IJCV, ToG, TIP, TNNLS, TMM, Computers & Graphics, TVCJ</li>
</ul>
<!-- === Activity Starts === -->


</td>
</tr>
</tbody>
</table>
<!-- === Homepage Ends === -->


<!-- Rendering Projects-->
<script type="text/javascript" src="./assets/project_renderer.js"></script>


<!-- Visitor Traffic -->
<script type="text/javascript" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=0&t=tt&d=ZoDwnZl26icJVfh0W6PRaWJC8UIjGa9CyHu2YaqgaRI&co=ffffff&ct=ffffff&cmo=ffffff&cmn=ffffff"></script>


</body>
</html>
